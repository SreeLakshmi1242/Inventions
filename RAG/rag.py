# -*- coding: utf-8 -*-
"""RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bggoX1vY73bciMI7sDE94-1QhBYLnoi-
"""

#import the packages
import os
import numpy as np
from getpass import getpass
from langchain import hub
from langchain_community.llms import HuggingFaceEndpoint
from langchain_community.chat_models.huggingface import ChatHuggingFace
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceHubEmbeddings
from langchain.prompts import PromptTemplate
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import PyPDFLoader
from langchain.chains import RetrievalQA
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from google.colab import files

"""#### **Authentication for Hugging Face API**"""

hfapi_key = getpass("Enter you HuggingFace access token:")
os.environ["HF_TOKEN"] = hfapi_key
os.environ["HUGGINGFACEHUB_API_TOKEN"] = hfapi_key

"""### **Loading the documents**


"""

# Load PDF
loaders = [
    # Duplicate documents on purpose
    PyPDFLoader("/content/pca_d1.pdf"),
    PyPDFLoader("/content/ens_d2.pdf"),
    PyPDFLoader("/content/ens_d2.pdf"),
]
docs = []
for loader in loaders:
    docs.extend(loader.load())

print(docs[0].page_content) # first document object in the list of documents

# Split
#from langchain.text_splitter import RecursiveCharacterTextSplitter
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size = 500,
    chunk_overlap = 50
)

splits = text_splitter.split_documents(docs)
print(len(splits))
splits





"""### **Embeddings**

Let's take our splits and embed them.
"""

embedding = HuggingFaceHubEmbeddings()

embedding

"""### **Vectorstores**

"""

persist_directory = 'docs/chroma/'
!rm -rf ./docs/chroma  # remove old database files if any

vectordb = Chroma.from_documents(
    documents=splits, # splits we created earlier
    embedding=embedding,
    persist_directory=persist_directory # save the directory
)

vectordb.persist() # Let's **save vectordb** so we can use it later!

print(vectordb._collection.count()) # same as number of splites


"""### **Retrieval + Question Answering :  Connecting with LLMs**

Connecting with LLMs typically involves initializing an instance of the chosen model and setting up parameters for its usage.

* In the following code, it merely assigns the name of the LLM to a variable and prints it.
* In this case, it will print "gpt-3.5-turbo", indicating the name of the chosen LLM.
"""

llm_name = "gpt-3.5-turbo"
print(llm_name)



"""### **Retrieval + Question Answering :  Connecting with LLMs**

"""

llm_name = "gpt-3.5-turbo"
print(llm_name)

question = "What is principal component analysis?"
docs = vectordb.max_marginal_relevance_search(question, k=2, fetch_k=3)
len(docs)

chat_llm = HuggingFaceEndpoint(
    repo_id="HuggingFaceH4/zephyr-7b-beta",
    task="text-generation",
    max_new_tokens = 512,
    top_k = 30,
    temperature = 0.1,
    repetition_penalty = 1.03,
)

llm = ChatHuggingFace(llm=chat_llm)

question = "What is principal component analysis?"

qa_chain = RetrievalQA.from_chain_type(llm, retriever=vectordb.as_retriever(), return_source_documents=True)

result = qa_chain.invoke({"query": question})

result["result"]

result["source_documents"]

"""###**RAG Prompt**"""

prompt = hub.pull("rlm/rag-prompt")
prompt

# Build prompt
template = """Use the following pieces of context to answer the question at the end.
If you don't know the answer, just say that you don't know, don't try to make up an answer.
Always say "thanks for asking!" at the end of the answer.
{context}
Question: {question}
Helpful Answer:"""
QA_CHAIN_PROMPT = PromptTemplate(input_variables=["context", "question"],template=template,)

QA_CHAIN_PROMPT

# Run chain
qa_chain = RetrievalQA.from_chain_type(llm,
                                       retriever=vectordb.as_retriever(search_type="mmr",search_kwargs={"k": 2, "fetch_k":6} ), # "k":2, "fetch_k":3
                                       chain_type_kwargs={"prompt": QA_CHAIN_PROMPT},
                                       return_source_documents=True
                                       )

qa_chain

"""**Example 1**"""

question = "What is principal component analysis?"
result = qa_chain.invoke({"query": question})
result["source_documents"]

result["result"]

"""**Example 2**"""

question = "What does it say about variance in context of both PCA and Ensemble?"
result = qa_chain({"query": question})
result["source_documents"]

result["result"]

"""### **Adding Memory**

"""

memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)

# Run chain
qa= ConversationalRetrievalChain.from_llm(llm,
                                       retriever=vectordb.as_retriever(search_type="mmr",search_kwargs={"k": 4, "fetch_k":8} ), # "k":2, "fetch_k":3
                                       memory=memory
                                       )

question = "tell me something about PCA"
result = qa.invoke({"question": question})

result['answer']

question = "please list point-wise,  how does pca works?"
result = qa({"question": question})

print(result['answer'])

question = "what do we get from covariance matrix for doing PCA?"
result = qa({"question": question})
print(result['answer'])
